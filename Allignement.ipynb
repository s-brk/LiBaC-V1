{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "                                      zoom=0.4559,\n",
    "                                      front=[0.6452, -0.3036, -0.7011],\n",
    "                                      lookat=[1.9892, 2.0208, 1.8945],\n",
    "                                      up=[-0.2779, -0.9482, 0.1556])\n",
    "    \n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    radius_normal = voxel_size * 2\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return pcd_down, pcd_fpfh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Load two point clouds and disturb initial pose.\n",
      ":: Downsample with a voxel size 0.100.\n",
      ":: Estimate normal with search radius 0.200.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      ":: Downsample with a voxel size 0.100.\n",
      ":: Estimate normal with search radius 0.200.\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      "930773\n",
      "94565822\n"
     ]
    }
   ],
   "source": [
    "def move_pointCloud_to_center(pointCloud):\n",
    "\n",
    "    center = pointCloud.get_center()\n",
    "    pointCloud.translate(-center, relative = True)\n",
    "\n",
    "    return pointCloud\n",
    "\n",
    "\n",
    "def prepare_dataset(voxel_size):\n",
    "    print(\":: Load two point clouds and disturb initial pose.\")\n",
    "\n",
    "    demo_icp_pcds = o3d.data.DemoICPPointClouds()\n",
    "    #cc_pc = o3d.io.read_point_cloud(demo_icp_pcds.paths[0])\n",
    "    #lidar_pc = o3d.io.read_point_cloud(demo_icp_pcds.paths[1])\n",
    "\n",
    "    #cc_pc = o3d.io.read_point_cloud(\"D:\\MASTERPROJEKT\\scripts\\Testfiles\\Masterkueche_GaussianSplat_CloudCompare.ply\")\n",
    "    cc_pc = o3d.io.read_point_cloud(\"D:\\MASTERPROJEKT\\scripts\\Testfiles\\Masterkueche_GaussianSplat_CloudCompare_bereinigt.ply\")\n",
    "    #lidar_pc = o3d.io.read_point_cloud(\"D:\\MASTERPROJEKT\\scripts\\Testfiles\\Masterkueche_LIDAR_CloudCompare_unalligned.ply\")\n",
    "    lidar_pc = o3d.io.read_point_cloud(\"D:\\MASTERPROJEKT\\scripts\\Testfiles\\Masterkueche_LIDAR_CloudCompare_unalligned_bereinigt.ply\")\n",
    "\n",
    "    cc_pc = move_pointCloud_to_center(cc_pc)\n",
    "    lidar_pc = move_pointCloud_to_center(lidar_pc)\n",
    "    \n",
    "    source = cc_pc\n",
    "    target = lidar_pc\n",
    "    trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "    source.transform(trans_init)\n",
    "    draw_registration_result(source, target, np.identity(4))\n",
    "\n",
    "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "    return source, target, source_down, target_down, source_fpfh, target_fpfh\n",
    "\n",
    "\n",
    "\n",
    "voxel_size = 0.1 # means 20cm for this dataset\n",
    "source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(\n",
    "    voxel_size)\n",
    "print(len(source.points))\n",
    "print(len(target.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: RANSAC registration on downsampled point clouds.\n",
      "   Since the downsampling voxel size is 0.100,\n",
      "   we use a liberal distance threshold 0.050.\n",
      "[Open3D WARNING] Too few correspondences (3528) after mutual filter, fall back to original correspondences.\n",
      "RegistrationResult with fitness=0.000000e+00, inlier_rmse=0.000000e+00, and correspondence_set size of 0\n",
      "Access transformation to get result.\n"
     ]
    }
   ],
   "source": [
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.5\n",
    "    normal_angle_threshold = 45\n",
    "    print(\":: RANSAC registration on downsampled point clouds.\")\n",
    "    print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
    "    print(\"   we use a liberal distance threshold %.3f.\" % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "        distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(True),\n",
    "        3, [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.99),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                distance_threshold),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnNormal(\n",
    "                normal_angle_threshold\n",
    "            )\n",
    "        ], o3d.pipelines.registration.RANSACConvergenceCriteria(10000, 1.0))\n",
    "    return result\n",
    "\n",
    "result_ransac = execute_global_registration(source_down, target_down,\n",
    "                                            source_fpfh, target_fpfh,\n",
    "                                            voxel_size)\n",
    "print(result_ransac)\n",
    "draw_registration_result(source_down, target_down, result_ransac.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Point-to-plane ICP registration is applied on original point\n",
      "   clouds to refine the alignment. This time we use a strict\n",
      "   distance threshold 0.080.\n",
      "RegistrationResult with fitness=7.858307e-02, inlier_rmse=4.593532e-02, and correspondence_set size of 73143\n",
      "Access transformation to get result.\n"
     ]
    }
   ],
   "source": [
    "def refine_registration(source, target, source_fpfh, target_fpfh, voxel_size):\n",
    "    source.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "    target.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "    distance_threshold = voxel_size * 0.4\n",
    "    print(\":: Point-to-plane ICP registration is applied on original point\")\n",
    "    print(\"   clouds to refine the alignment. This time we use a strict\")\n",
    "    print(\"   distance threshold %.3f.\" % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, distance_threshold, result_ransac.transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "    return result\n",
    "\n",
    "result_icp = refine_registration(source, target, source_fpfh, target_fpfh,\n",
    "                                 voxel_size)\n",
    "print(result_icp)\n",
    "draw_registration_result(source, target, result_icp.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Apply fast global registration with distance threshold 0.050\n",
      "Fast global registration took 12.056 sec.\n",
      "\n",
      "RegistrationResult with fitness=1.698881e-02, inlier_rmse=3.757945e-02, and correspondence_set size of 973\n",
      "Access transformation to get result.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def execute_fast_global_registration(source_down, target_down, source_fpfh,\n",
    "                                     target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.5\n",
    "\n",
    "    print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "            % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh,\n",
    "        o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "            maximum_correspondence_distance=distance_threshold))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "result_fast = execute_fast_global_registration(source_down, target_down,\n",
    "                                               source_fpfh, target_fpfh,\n",
    "                                               voxel_size)\n",
    "print(\"Fast global registration took %.3f sec.\\n\" % (time.time() - start))\n",
    "print(result_fast)\n",
    "draw_registration_result(source_down, target_down, result_fast.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Point-to-plane ICP registration is applied on original point\n",
      "   clouds to refine the alignment. This time we use a strict\n",
      "   distance threshold 0.080.\n",
      "RegistrationResult with fitness=3.544272e-02, inlier_rmse=5.999299e-02, and correspondence_set size of 564\n",
      "Access transformation to get result.\n"
     ]
    }
   ],
   "source": [
    "result_icp = refine_registration(source_down, target_down, source_fpfh, target_fpfh,\n",
    "                                 voxel_size)\n",
    "print(result_icp)\n",
    "draw_registration_result(source, target, result_icp.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "from matplotlib import pyplot as pyplot\n",
    "import numpy as np\n",
    "import copy\n",
    "import scipy\n",
    "from scipy import spatial \n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "\n",
    "#Kabsch Algorithm\n",
    "def compute_transformation(source,target):\n",
    "    #Normalization\n",
    "    number = len(source)\n",
    "    #the centroid of source points\n",
    "    cs = np.zeros((3,1))\n",
    "    #the centroid of target points\n",
    "    ct = copy.deepcopy(cs)\n",
    "    cs[0] = np.mean(source[:][0]);cs[1]=np.mean(source[:][1]);cs[2]=np.mean(source[:][2])\n",
    "    ct[0] = np.mean(target[:][0]);cs[1]=np.mean(target[:][1]);cs[2]=np.mean(target[:][2])\n",
    "    #covariance matrix\n",
    "    cov = np.zeros((3,3))\n",
    "    #translate the centroids of both models to the origin of the coordinate system (0,0,0)\n",
    "    #subtract from each point coordinates the coordinates of its corresponding centroid\n",
    "    for i in range(number):\n",
    "        sources = source[i].reshape(-1,1)-cs\n",
    "        targets = target[i].reshape(-1,1)-ct\n",
    "        cov = cov + np.dot(sources,np.transpose(targets))\n",
    "    #SVD (singular values decomposition)\n",
    "    u,w,v = np.linalg.svd(cov)\n",
    "    #rotation matrix\n",
    "    R = np.dot(u,np.transpose(v))\n",
    "    #Transformation vector\n",
    "    T = ct - np.dot(R,cs)\n",
    "    return R, T\n",
    "\n",
    "#compute the transformed points from source to target based on the R/T found in Kabsch Algorithm\n",
    "def _transform(source,R,T):\n",
    "    points = []\n",
    "    for point in source:\n",
    "        points.append(np.dot(R,point.reshape(-1,1)+T))\n",
    "    return points\n",
    "\n",
    "#compute the root mean square error between source and target\n",
    "def compute_rmse(source,target,R,T):\n",
    "    rmse = 0\n",
    "    number = len(target)\n",
    "    points = _transform(source,R,T)\n",
    "    for i in range(number):\n",
    "        error = target[i].reshape(-1,1)-points[i]\n",
    "        rmse = rmse + math.sqrt(error[0]**2+error[1]**2+error[2]**2)\n",
    "    return rmse\n",
    "\n",
    "def draw_registrations(source, target, transformation = None, recolor = False):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    if(recolor): # recolor the points\n",
    "        source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "        target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    if(transformation is not None): # transforma source to targets\n",
    "        source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "def pc2array(pointcloud):\n",
    "    return np.asarray(pointcloud.points)\n",
    "\n",
    "def registration_RANSAC(source,target,source_feature,target_feature,ransac_n=3,max_iteration=100000,max_validation=100):\n",
    "    #the intention of RANSAC is to get the optimal transformation between the source and target point cloud\n",
    "    s = pc2array(source) #(4760,3)\n",
    "    t = pc2array(target)\n",
    "    #source features (33,4760)\n",
    "    sf = np.transpose(source_feature.data)\n",
    "    tf = np.transpose(target_feature.data)\n",
    "    #create a KD tree\n",
    "    tree = spatial.KDTree(tf)\n",
    "    corres_stock = tree.query(sf)[1]\n",
    "    for i in range(max_iteration):\n",
    "        #take ransac_n points randomly\n",
    "        idx = [random.randint(0,s.shape[0]-1) for j in range(ransac_n)]\n",
    "        corres_idx = corres_stock[idx]\n",
    "        source_point = s[idx,...]\n",
    "        target_point = t[corres_idx,...]\n",
    "        #estimate transformation\n",
    "        #use Kabsch Algorithm\n",
    "        R, T = compute_transformation(source_point,target_point)\n",
    "        #calculate rmse for all points\n",
    "        source_point = s\n",
    "        target_point = t[corres_stock,...]\n",
    "        rmse = compute_rmse(source_point,target_point,R,T)\n",
    "        #compare rmse and optimal rmse and then store the smaller one as optimal values\n",
    "        if not i:\n",
    "            opt_rmse = rmse\n",
    "            opt_R = R\n",
    "            opt_T = T\n",
    "        else:\n",
    "            if rmse < opt_rmse:\n",
    "                opt_rmse = rmse\n",
    "                opt_R = R\n",
    "                opt_T = T\n",
    "    return opt_R, opt_T\n",
    "\n",
    "#used for downsampling\n",
    "voxel_size = 0.05\n",
    "#this is to get the fpfh features, just call the library\n",
    "def get_fpfh(cp):\n",
    "    cp = cp.voxel_down_sample(voxel_size)\n",
    "    cp.estimate_normals()\n",
    "    return cp, o3d.pipelines.registration.compute_fpfh_feature(cp, o3d.geometry.KDTreeSearchParamHybrid(radius=5, max_nn=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = o3d.io.read_point_cloud(\"D:\\MASTERPROJEKT\\scripts\\Testfiles\\Masterkueche_GaussianSplat_CloudCompare.ply\")\n",
    "target = o3d.io.read_point_cloud(\"D:\\MASTERPROJEKT\\scripts\\Testfiles\\Masterkueche_LIDAR_CloudCompare_unalligned.ply\")\n",
    "#if we want to use RANSAC registration, get_fpfh features should be acquired firstly                 \n",
    "r1, f1 = get_fpfh(source)\n",
    "r2, f2 = get_fpfh(target)\n",
    "R, T = registration_RANSAC(r1,r2,f1,f2)\n",
    "#transformation matrix is formed by R, T based on np.hstack and np.vstack(corporate two matrices by rows)\n",
    "#Notice we need add the last row [0 0 0 1] to make it homogeneous \n",
    "transformation = np.vstack((np.hstack((np.float64(R), np.float64(T))), np.array([0,0,0,1])))\n",
    "draw_registrations(r1, r2, transformation, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcloudedit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
